%\documentclass[10pt,landscape]{article}
\documentclass[paper=letter,fontsize=3mm]{scrartcl}

\renewcommand{\familydefault}{\sfdefault}


\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{centernot}
\usepackage{amsmath,amssymb,amsthm}

\linespread{1}

% Math Operators
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Prob}{P}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% Math Commands 
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\ind}{\stackrel{\text{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\eqdist}{\stackrel{\text{d}}{=}}
\newcommand{\convdist}{\stackrel{\text{d}}{\longrightarrow}}
\newcommand{\convprob}{\stackrel{\text{p}}{\longrightarrow}}
\newcommand{\convas}{\stackrel{\text{a.s.}}{\longrightarrow}}
\newcommand{\convL}[1]{\stackrel{\mathcal{L}_{#1}}{\longrightarrow}}
\newcommand{\Norm}{\mathcal{N}} 
\newcommand{\Borel}{\mathcal{B}}
\newcommand{\eps}{\varepsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand\indicate[1]{\mathbb{I}_{ #1 }}
\newcommand\abs[1]{\left| #1 \right|}
\newcommand\norm[1]{\left\lVert #1 \right\rVert}
\newcommand\inner[1]{\left\langle #1 \right\rangle}
\newcommand\set[1]{\left\{ #1 \right\}}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.2in,left=.25in,right=.25in,bottom=.2in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother


% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.01ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\scriptsize
\begin{multicols*}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{.05pt}
\setlength{\postmulticols}{.05pt}
\setlength{\multicolsep}{.05pt}
\setlength{\columnsep}{.05pt}

\subsection*{Laws of Large Numbers}
$\set{X_n}$ obeys the LLN if $\exists \set{b_n} \subset \R$ and $0 < a_n \uparrow$ such that
$$\text{\bf{SLLN}:} \frac{S_n - b_n}{a_n} \convas 0 \quad \text{\bf{WLLN}:} \frac{S_n - b_n}{a_n} \convprob 0 .$$

\textbf{Kronecker's Lemma}:  If $\set{a_n}, \set{b_n} \subset \R$ such that $0<b_n \uparrow \infty$ and $\sum_{n=1}^\infty a_n/b_n$ converges, then
$$ \frac{1}{b_n} \sum_{j=1}^n a_n \to 0 \text{ as } n \to \infty.$$

\textbf{Cesaro's Mean Summability Theorem}: If $\set{x_n} \subset \R$ such that $\lim_{n\to\infty}x_n=x<\infty$, then 
$$\lim_{n\to\infty} \frac{1}{n}\sum_{j=1}^nx_j = x.$$

\textbf{Theorem 4.14}: If $\set{X_n}$ ind such that $\sum_{n=1}^\infty \E\abs{X_n}^{\alpha_n}/n^{\alpha_n} < \infty$ for $\alpha_n \in [1,2]$, then
$$\frac{S_n - \E S_n}{n} = \frac{1}{n}\sum_{i=1}^n (X_i - \E X_i) \convas 0.$$

\textbf{Marcinkiewicz-Zygmund SLLN}: Let $\set{X_n}$ be iid, $S_n = \sum_{j=1}^nX_n$, and $p \in (0,2)$.
\begin{enumerate}
\item If $\frac{S_n -nc}{n^{1/p}}\convas 0$ for some $c \in \R$, then $\E\abs{X_1}^p < \infty$.
\item If $\E\abs{X_1}^p < \infty$, then (2) holds with $c = \E X_1$ if $p \in [1,2)$ and (2) holds for any $c\in \R$ if $p \in (0,1)$.
\end{enumerate}

\textbf{Kolmogorov's SLLN}: If $\set{X_n}$ are iid, then 
$$\bar{X}_n = \frac{S_n}{n} \convas \E X_1 \iff \E\abs{X_1} < \infty \iff \frac{S_n - n\E X_1}{n} \convas 0.$$

\textbf{Useful Theorem}: For any r.v.\@  $X$ and $r > 0$,
$$\sum_{n=1}^\infty P(\abs{X} > n^{1/r}) \le \E\abs{X}^r \le \sum_{n=0}^\infty P(\abs{X} > n^{1/r}).$$

\textbf{Etemaldi's SLLN}: If $\set{X_n}$ are \emph{pairwise} ind and identically distributed, then 
$$\bar{X}_n = \frac{S_n}{n} \convas \E X_1 \iff \E\abs{X_1} < \infty.$$

\textbf{Theorem 4.18} (general WLLN): $\set{X_n}$ ind and put $S_n = \sum_{j=1}^n X_j$. If
$$ \sum_{j=1}^n P(\abs{X_j} > n) \to 0 \quad \text{ and } \quad \frac{1}{n^2} \sum_{j=1}^n E X_j^{(n)2} \to 0,$$
then $$\frac{S_n-a_n}{n}\convprob 0,$$
where $a_n = \sum_{j=1}^n \E X_j^{(n)}$ and $X_j^{(n)} \equiv X_j I(\abs{X_j} \le n).$ \\ \medskip

\textbf{[Corollary] Feller's WLLN} (without a 1st moment hypothesis): if $\set{X_n}$ iid with $\lim_{n\to\infty} xP(\abs{X_1}>x) = 0$, then $$\frac{S_n}{n} - \E X_1^{(n)} \convprob 0.$$


\subsection*{Empirical Distributions}
The \textbf{empirical cdf} of $X_1, \dots, X_n$ is the random cdf is the proportion of observations no larger than a fixed $x$:
$$F_n(x) = \frac{1}{n} \sum_{i=1}^n I(X_i \le x), \quad x \in \R.$$

\begin{itemize}
\item With $X_i$'s on $(\Omega, \mathcal{F}, P)$, for each $\omega \in \Omega$,
$$F_n(x, \omega) = \frac{1}{n} \sum_{i=1}^n I(X_i(\omega) \le x).$$
\item $F_n(x)$ is a right-continuous, nondecreasing function of $x \in \R$,
\item For any $x \in \R$, $F_n(x)$ is a r.v., i.e., is $\langle \mathcal{F}, \Borel(\R)\rangle$-mble:
$$F_n(x) = \frac{1}{n}\sum_{i=1}^n I(X_i^{-1}(-\infty,x])(\omega).$$
\end{itemize}

\textbf{Glivenko-Cantelli Theorem}: $\set{X_n}$ iid with cdf $F(\cdot)$. Let $F_n(\cdot)$ be the empirical cdf based on $X_1, \dots, X_n$ and define 
$$D_n = \sup_{x\in\R} \abs{F_n(x) - F(x)}.$$
Then, (i) $D_n$ is a r.v.\@ for any $n \ge 1$ and (ii) $D_n \convas 0$ as $n \to \infty$. \\ \medskip

\textbf{Quantile function}: $\phi(u) = \inf\set{x \in \R: F(x) \ge u} \equiv F^{-1}(u), ~u \in (0,1)$ which implies
$$F(x) \ge u \iff x \ge \phi(u) \quad \text{ and } \quad F(\phi(u)-) \le u \le F(\phi(u)).$$

\subsection*{Convergence in Distribution}: Let $\mu_n, n\ge0$ be probability measures on $(\R^k, \Borel(\R^k))$ for some $1 \le k < \infty$.
\begin{enumerate}
\item For $\mathbf{x} = (x_1, \dots, x_k) \in \R^k$, the \textbf{cdf} of $\mu_n$ is
$$F_n(\mathbf{x}) = \mu_n\big( (-\infty, x_1] \times \cdots \times (-\infty, x_n]\big).$$
If a random vector $X_n$ has probability distribution $\mu_n$ [i.e., $P(X_n \in A) = \mu_n(A), ~A \in \Borel(\R^k)$], then $F_n$ is also called the cdf of $X_n$. 
\item A sequence of probability measures $\mu_n$ (or corresponding cdfs $F_n$) \textbf{converges weakly} to $\mu_o$ (to $F_0$), denoted as $\mu_n \Rightarrow \mu_0$ (or as $F_n \Rightarrow F_0$), if
$$\lim_{n\to\infty}F_n(\mathbf{x}) = F_0(\mathbf{x}) \quad \forall \mathbf{x} \in C(F_0),$$
where $C(F_0) = \set{\mathbf{x} \in \R^k: F_0 \text{ is continuous at } \mathbf{x}}$. 
\item A sequence of random vectors $X_n$ in $R^k$ (with distributions $\mu_n$) \textbf{converges in distribution (law)} to a random variable $X_0$ (with distribution $\mu_0$) if $\mu_n \Rightarrow \mu_0$, denoted by $X_n \convdist X_0$. \\
That is, if $X_n = (X_{n,1}, \dots, X_{n,i})$ has cdf $F_n, n \ge 0$, then
\begin{align*}
\lim_{n\to\infty}F_n(\mathbf{x}) 
&= \lim_{n\to\infty} P(X_{n,1} \le x_1, \dots, X_{n,k} \le x_k) \\
&= P(X_{0,1} \le x_1, \dots, X_{0,k} \le x_k) \\
&= F_0(\mathbf{x}) \quad \forall \mathbf{x} \in C(F_0).
\end{align*}
Note that
\begin{itemize}
\item $\mathbf{x} = (x_1, \dots, x_k) \in C(F_0) \iff F_0(\mathbf{x}) = P(X_{0,1} < x_1, \dots, X_{0,k} < x_k) = F_0(\mathbf{x}-)$
i.e., if also left continuous. 
\item $C(F_0)^c$ is at most countable.
\item $X_n \convprob X_0 \implies X_n \convdist X_0$ but not the other direction, unless $X_0$ is degenerate.
\end{itemize}
\end{enumerate}

\textbf{Skorohod's Embedding Theorem}: If $\mu_n, n\ge0$ are probability measures on $(\R^k, \Borel(\R^k))$ for some $1\le k < \infty$ such that $\mu_n \Rightarrow \mu_0$, then $\exists$ random vectors $\set{Y_n}_{n\ge0}$ on a \emph{common} probability space such that $Y_n$ has probability distribution $\mu_n$ for all $n \ge 0$ and $Y_n \convas Y_0$. That is, $P(Y_n \in A) = \mu_n(A), ~A \in \Borel(\R^k), n \ge 0$. \\ \medskip

\textbf{Continuous Mapping Theorem}: \\
\underline{Version (a)}: Let $\mu_n, n \ge0$ be probability measures on $(\R^k, \Borel(\R^k))$ for $1 \le k < \infty$ and let $h: \R^k \to \R^m$ for $1 \le m < \infty$ be a $\langle \Borel(\R^k), \Borel(\R^m) \rangle$-mble function such that $\mu_0(D_h)  = 0$, where $D_n \in \Borel(\R^k)$ denotes the set of all points of discontinuities of the function $h$. If $\mu_n \Rightarrow \mu_0$, then the induced measures converge weakly: $$\mu_n h^{-1} \Rightarrow \mu_0 h^{-1}.$$
\underline{Version (b)}: Let $X_n, n \ge 0$ be $R^k$-valued random vectors and let measurable $h: \R^k \to \R^m$ be such that $P(X_0 \in D_h) = 0$, where $D_h$ is as above. If $X_n \convdist X_0$, then
$$h(X_n) \convdist h(X_0).$$

\underline{Corollary}: If $X_n, Y_n, n \ge 0$ be r.v.'s such that $(X_n, Y_n) \convdist (X_0, Y_0)$, then
$$X_n + Y_n \convdist X_0 + Y_0, \quad X_nY_n \convdist X_0Y_0, \quad X_n/Y_n \convdist X_0/Y_0 \text{ if } P(Y_0 = 0) = 0.$$

\underline{Corollary (Slutsky's Theorem)}: If $X_n, Y_n, n \ge 1$ be r.v.'s such that $X_n \convdist X$ and $Y_n \convprob a$ for some $a \in \R$, then
$$X_n + Y_n \convdist X + a, \quad X_nY_n \convdist aX, \quad X_n/Y_n \convdist X/a \text{ if } a \ne 0.$$

\subsection*{Characterizations of Convergence in Distribution}
For a probability measure $\mu$ on $(\R^k, \Borel(\R^k))$, a set $A \in \Borel(\R^k)$ is called a \textbf{$\mu$-continuity} set if $\mu(\partial A) = 0$, where $\partial A = \overline{A} \setminus \text{int}A$. E.g., $$\partial (-\infty, x] = (-\infty, x] \setminus (-\infty, x) = \set{x}.$$

\textbf{Helly-Bray ``Portmanteau'' Theorem}: If $\mu_n, n\ge0$ are probability measures on $(\R, \Borel(\R))$, then 
\begin{enumerate}
\item $\mu_n \Rightarrow \mu_0 \iff \mu_n(A) \to \mu_0(A) ~\forall A \in \Borel(\R) \ni \mu_0(\partial A) = 0.$
\item $\mu_n \Rightarrow \mu_0 \iff \int fd\mu_n \to \int f d\mu_0$ for all bounded continuous functions $f: \R \to \R$.
\end{enumerate}
Remarks:
\begin{itemize}
\item $\mu_n \Rightarrow \mu_0 \not\implies \mu_n(A) \to \mu_0(A) \forall A \in \Borel(\R^k)$.
\item Holds for $(\R^k, \Borel(\R^k))$.
\item Can generalize to a metric space $(S,d)$.
\end{itemize}
\textbf{Lemma 5.8}: If $\mu_n \Rightarrow \mu_0$ on $(\R, \Borel(\R))$ and $f: \R \to \R$ is a bounded, Borel-mble function with $\mu_0(D_f)$ (where $D_f \in \Borel(\R)$ is the set of discontinuity points of $f$), then $$\int fd\mu_n \to \int f d\mu_0 \text{ as } n \to \infty.$$

A sequence of probability measures $\set{\mu_n}$ on $(\R^k, \Borel(\R^k))$ is \textbf{tight} if $\forall \eps >0, \exists M_\eps > 0 $ such that
$$\sup_{n\ge1} \mu_n\left(\set{x\in\R^k: \norm{x} > M_\eps}\right) < \eps.$$
For a single probability measure on $\R$, given $\eps$, we can find $M_\eps$ such that $\mu([-M_\eps, M_\eps]) < \eps$ and note $\mu([-M_\eps, M_\eps]) \uparrow \mu(\R) = 1$. \\ \medskip

A sequence of $R^k$-valued random vectors $\set{X_n}$ is \textbf{tight} or \textbf{stochastically bounded} if their corresponding $\set{\mu_n}$ is tight. That is, $\forall \eps > 0, \exists M_\eps > 0$ such that
$$\sup_{n\ge1}P(\norm{X_n} > M_\eps) = \sup_{n\ge1}\mu_n\left(\set{x \in \R^k: \norm{x} > M_\eps }\right) < \eps,$$
where $\mu_n(A) = P(X_n \in A), ~ A \in \Borel(\R^k)$. \\ \medskip

A sequence of random vectors $\set{X_n}$ is \textbf{uniformly integrable} if $\forall \eps > 0, \exists t_\eps > 0$ such that
$$\sup_{n\ge1}\E \norm{X_n} I(\norm{X_n} > t_\eps) = \sup_{n\ge1}\int_{\norm{x} > t_\eps} \norm{x} d\mu_n < \eps,$$
where $\mu_n(A) = P(X_n \in A), ~ A \in \Borel(\R^k)$. \\ \medskip

\textbf{Proposition 5.9}: Let $\set{X_n}$ be r.v.'s.
\begin{enumerate}
\item If $X_n \convdist X_0$, then $\set{X_n}$ is tight. 
\item If $\set{X_n}$ is tight and $Y_n \convprob 0$ for $X_n, Y_n$ defined on $(\Omega_n, \mathcal{F}_n, P_n)$, then $X_nY_n \convprob 0$.
\item But, weak convergence ``almost'' implies tightness - see Prokhorov's theorem.
\end{enumerate}

\textbf{Theorem 5.10}: A sequence of r.v.'s $\set{X_n}$ (or probability measures $\set{\mu_n})$ is tight iff for any subsequence $X_{n_k}$ of $X_n$ there exists a further subsequence $X_{n_{k_j}}$ of $X_{n_k}$ and a r.v.\@ (or probability measure $\mu_0$) such that $X_{n_{k_j}} \convdist X_0$ (or $\mu_{n_{k_j}} \Rightarrow \mu_0$). Note: $X_0$ (or $\mu_0$) depends on the particular subsequence $X_{n_k}$.  \\ \medskip

\textbf{Corollary}: If $\set{X_n}$ is tight and all its convergent subsequences converge in distribution to the \emph{same} r.v.\@ $X_0$, then $X_n \convdist X_0$.  \\ \medskip

\textbf{Theorem 5.12} (conv in dist + UI $\implies$ conv in mean): If $\set{X_n}, n\ge1$ is UI and $X_n \convdist X_0$, then $\E\abs{X_0} < \infty$ and $\E X_n \to \E X_0$. \\ \medskip

\textbf{Corollary 5.13}
If $X_n \convdist X_0$ and $\sup_{n\ge1} \E\abs{X_n}^{r+\delta} < \infty$ for some integer $r \ge 1$ and real $\delta > 0$, then $\E\abs{X_0}^r < \infty$ and $\E X_n^r \to \E X_0^r$ (recall that $\sup_{n\ge1} \E\abs{Z_n}^{1+\delta} \implies \set{Z_n}$ is UI). \\ \medskip 

\textbf{Fr\'{e}chet-Shohat Theorem}: If $\lim_{n\to\infty} \E X_n^r = \beta_r \in \R$ for all integers $r \ge 1$ and if $\set{\beta_r: r \ge 1}$ are the moments of a \emph{unique} r.v.\@ $X_0$, then $X_n \convdist X_0$. \\ \medskip

\textbf{Moments uniquely determine distribution} when Cardeman's condition is met, $\sum_{r=1}^\infty \beta_{2r}^{-1/(2r)} = \infty,$ or if the MGF $M_X(t) = \E e^{tX} < \infty ~ \forall \abs{t} < \eps$ for some $\eps > 0$. Recall: $$\E X^r = \frac{d^r}{dt^r} M_X(t) \bigg|_{t=0}.$$

\subsection*{Characteristic Functions}
A \textbf{complex} number is $a + ib$, where $a,b \in \R$ and $i = \sqrt{-1}$. If $a + bi$ and $c + di$ are complex, then their sum is $(a + b) + (c+d)i$, their product is $(ac - bd) + (ad + bc)i$, and the modulus is $\abs{a + bi} = \sqrt{a^2 + b^2} = \sqrt{(a+bi)(a-bi)}$. IMPORTANT! For any $b \in \R$,
$$e^{bi} = \cos(b) + i \sin(b)$$
and $\abs{e^{bi}} = \sqrt{\cos^2(b) + \sin^2(b)} =1$ and $e^{ai}e^{bi} = e^{(a+b)i}$ for $a,b\in\R$. ALSO, for fixed $b \in \R$, the function $g(t) = e^{tbi}: \R \to \C$ is infinitely differentiable in $t$ with $n$th derivative $(bi)^n e^{tbi}$. \\ \medskip

For a random vector $X$ in $\R^k$, the \textbf{characteristic function} (CF) is defined as
$$\phi_X(t) = \E e^{it'X} = \E \cos(t'X) + i \E \sin(t'X), \quad t\in \R^k, i = \sqrt{-1}.$$
It allows for easy convolutions (same as MGF), always exists, uniquely identifies distribution, and p.w. convergence implies weak convergence. \\ \medskip

Note that $\phi_X(0) = 1$ and $\phi_X(t)$ is uniformly continuous on $\R^k$: by the BCT,
\begin{align*}
\sup_{t\in\R^k} \abs{\phi_X(t+h) - \phi_X(t)}
&=  \sup_{t\in\R^k} \abs{\E e^{i(t+h)'X} - \E e^{it'X}} \\
&\le \E \abs{e^{ih'X - 1}} \\
&= \int_{R^k} \abs{e^{ih'x} - 1} d\mu_X(x) \to 0 \text{ as } \abs{h} \to 0.
\end{align*}

\textbf{Theorem 5.15}: If $X$ is a r.v.\@ with $\E \abs{X}^r < \infty$ for some $r \ge 1$, then $\phi_X(t)$ is $r$-times differentiable on $\R$ and
$$\phi^{(r)}_X(t) = \E (iX)^r e^{itX}, \quad t \in \R.$$

\textbf{Riemann-Lebesgue Lemma}: If the distribution of a r.v.\@ $X$ has a density $f$ w.r.t.\@ the Lebesgue measure on $\R$, then $\phi_X(t) \to 0$ as $\abs{t} \to \infty$. \\ \medskip

\textbf{Levy Inversion Formula} (use CF to recover dist): If $X$ is a r.v.\@ with CF $\phi_X$, then for any $a,b \in \R$ with $P(X = a) = 0 = P(X = b)$,
$$P(a < X \le b) = \lim_{T\to\infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-ita} - e^{-itb}}{it}\phi_X(t)dt.$$

\textbf{Corollary}: If we also assume $\int \abs{\phi_X(t)}dt<\infty$ (which implies $C(F) = \R$), then $X$ has a pdf $f$ w.r.t.\@ the Lebesgue measure on $\R$ given by
$$f_X(x) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{-itx}\phi_X(t)dt.$$

\textbf{Levy Continuity Theorem}: suppose $\set{X_n}$ is a sequence of r.v.'s each with CF $\phi_{X_n}$
\begin{enumerate}
\item If $X_n \convdist X_o$, then for any $T > 0$,
$$\sup_{\abs{t} < T} \abs{\phi_{X_n}(t) - \phi_{X_0}(t)} \to 0 \text{ as } n \to \infty.$$
\item If $\phi_{X_n}(t) \to g(t)$ as $n \to \infty$ for all $t \in \R$ and $g(\cdot)$ is continuous at zero, then $g(\cdot)$ is a CF and $X_n \convdist X_0$, where $X_0$ is the r.v.\@ with CF $g(\cdot)$.
\end{enumerate}

\textbf{Corollary 5.20}: $X_n \convdist X_0 \iff \phi_{X_n}(t) \to \phi_{X_0}(t)$ as $n \to \infty$ for all $t \in \R$. \\ \medskip

\textbf{Inversion Formula in $\R^k$}: Let $X$ be a $\R^k$-valued random vector with CF $\phi_X(t)$ for $t = (t_1, \dots, t_k) \in \R^k$. Then, for any rectangle $A = (a_1, b_1] \times \cdots \times (a_k, b_k]$ with $P(X \in \partial A) = 0$,
$$P(X \in A) = \lim_{T\to\infty} \frac{1}{(2\pi)^k} \int_{-T}^T \cdots \int_{-T}^T \prod_{j=1}^k
\frac{e^{-it_ja_j} - e^{-it_jb_j}}{it_j} \phi_X(t_1, \dots, t_k)dt_1\dots dt_k.$$

Also, if $\int_{\R^k} \abs{\phi_X(t_1,\dots,t_k)}dt_1\dots dt_k < \infty$, then $X$ has a bounded, continuous density $f_X(x)$ w.r.t.\@ the Lebesgue measure in $\R^k$ given by
$$f_X(x) = \frac{1}{(2\pi)^k} \int_{\R^k} e^{-i\sum_{j=1}^k x_jt_j}\phi_X(t_1, \dots, t_k)dt_1 \cdots dt_k, \quad x = (x_1, \dots, x_k) \in \R^k.$$

\textbf{Theorem 5.22}: On a psp $(\Omega, \mathcal{F}, P)$, r.v.'s $X_1, \dots, X_k$ are ind iff for all $t_1, \dots, t_k \in \R$,
$$\phi_{X_1, \dots, X_k}(t_1, \dots, t_k) \equiv \E e^{i\sum_{j=1}^k X_j t_j} =
\prod_{j=1}^k \E e^{iX_j t_j} = \prod_{j=1}^k \phi_{X_j}(t_j).$$


\textbf{Theorem 5.23}: For a sequence $\set{X_n}, n \ge 0$ of $\R^k$-valued random vectors,
\begin{enumerate}
\item $X_n \convdist X_0 \iff \phi_{X_n}(t) \to \phi_{X_0}(t) ~ \forall t \in \R^k$,
\item  (Cramer-Wold device) $X_n \convdist X_0 \iff t'X_n \convdist t' X_0 ~ \forall t \in \R^k$,
\end{enumerate}



























\end{multicols*}
\end{document}